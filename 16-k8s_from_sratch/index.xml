<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>K8S From Scratch on Hugo Relearn Theme</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/index.html</link><description>Recent content in K8S From Scratch on Hugo Relearn Theme</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 29 Dec 2018 17:15:52 +0000</lastBuildDate><atom:link href="https://kubernetes.ansilh.com/16-k8s_from_sratch/index.xml" rel="self" type="application/rss+xml"/><item><title>API Access Control</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/01-api-access-controll/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/01-api-access-controll/index.html</guid><description>Users access the API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram:
Authentication Once TLS is established, the HTTP request moves to the Authentication step. This is shown as step 1 in the diagram.
We use X509 Client Certs for authentication.</description></item><item><title>Pre-requisites</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/02-vm-creation/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/02-vm-creation/index.html</guid><description>Install an Ubuntu 16.04 VM and create 4 clones from it. Make sure to create a user called k8s which will be used in upcoming steps. Names should contain your initials for identification purpose if you are using a shared environment. Do not change any VM parameters except name while cloning. Once cloning completes , start all VMs. Make sure to give the hostname with prefix k8s-master- for Master and k8s-worker- for worker If you miss this , then the scripts/command may fail down the line.</description></item><item><title>Tools</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/03-certificates-client-tools/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/03-certificates-client-tools/index.html</guid><description>Logon to master node and follow below steps
1. Install cfssl to generate certificates $ wget -q --show-progress --https-only --timestamping \ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64$ chmod +x cfssl_linux-amd64 cfssljson_linux-amd64$ sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl$ sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson Verification $ cfssl version Output Version: 1.2.0 Revision: dev Runtime: go1.6 2. Install kubectl Download kubectl wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubectl Make it executable and move to one of the shell $PATH $ chmod +x kubectl $ sudo mv kubectl /usr/local/bin/ Verification $ kubectl version --client Output Client Version: version.</description></item><item><title>CA Configuration</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/04-certificate-ca/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/04-certificate-ca/index.html</guid><description>PKI Infrastructure We will provision a PKI Infrastructure using CloudFlare&amp;rsquo;s PKI toolkit, cfssl, then use it to bootstrap a Certificate Authority, and generate TLS certificates for the following components: etcd, kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, and kube-proxy.
Certificate Authority In cryptography, a certificate authority or certification authority (CA) is an entity that issues digital certificates.
Generate CA default files (To understand the structure of CA and CSR json . We will overwrite this configs in next steps) $ cfssl print-defaults config &amp;gt; ca-config.</description></item><item><title>Client and Server Certificates</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/05-client-and-server-certs/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/05-client-and-server-certs/index.html</guid><description>In this section you will generate client and server certificates for each Kubernetes component and a client certificate for the Kubernetes admin user.
The Admin Client Certificate (This will be used for kubectl command) $ { cat &amp;gt; admin-csr.json &amp;lt;&amp;lt;EOF { &amp;#34;CN&amp;#34;: &amp;#34;admin&amp;#34;, &amp;#34;key&amp;#34;: { &amp;#34;algo&amp;#34;: &amp;#34;rsa&amp;#34;, &amp;#34;size&amp;#34;: 2048 }, &amp;#34;names&amp;#34;: [ { &amp;#34;C&amp;#34;: &amp;#34;IN&amp;#34;, &amp;#34;L&amp;#34;: &amp;#34;Bangalore&amp;#34;, &amp;#34;O&amp;#34;: &amp;#34;system:masters&amp;#34;, &amp;#34;OU&amp;#34;: &amp;#34;Kubernetes from Scratch&amp;#34;, &amp;#34;ST&amp;#34;: &amp;#34;Karnataka&amp;#34; } ] } EOF $ cfssl gencert \ -ca=ca.</description></item><item><title>Configuration Files</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/06-configuration-files/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/06-configuration-files/index.html</guid><description>In this lab you will generate Kubernetes configuration files, also known as kubeconfigs, which enable Kubernetes clients to locate and authenticate to the Kubernetes API Servers.
Client Authentication Configs In this section you will generate kubeconfig files for the controller manager, kubelet, kube-proxy, and scheduler clients and the admin user.
Kubernetes Public IP Address Each kubeconfig requires a Kubernetes API Server to connect to. Set the KUBERNETES_PUBLIC_ADDRESS with the IP of master.</description></item><item><title>Etcd Bootstrap</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/07-bootstrap-etcd/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/07-bootstrap-etcd/index.html</guid><description>Bootstrapping the etcd Cluster Kubernetes components are stateless and store cluster state in etcd. In this lab you will bootstrap a three node etcd cluster and configure it for high availability and secure remote access.
Prerequisites The commands in this lab must be run on each worker instance: Login to each controller instance using the k8s user. Example:
ssh k8s-worker-ah-02Running commands in parallel with tmux tmux can be used to run commands on multiple compute instances at the same time.</description></item><item><title>Control Plane Setup</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/08-control-plane/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/08-control-plane/index.html</guid><description>Bootstrapping the Kubernetes Control Plane In this lab you will bootstrap the Kubernetes control plane across three compute instances and configure it for high availability. You will also create an external load balancer that exposes the Kubernetes API Servers to remote clients. The following components will be installed on each node: Kubernetes API Server, Scheduler, and Controller Manager.
Prerequisites The commands in this lab must be run only on master node (eg: k8s-master-ah-01)</description></item><item><title>Worker Plane Setup</title><link>https://kubernetes.ansilh.com/16-k8s_from_sratch/09-worker-plane/index.html</link><pubDate>Sat, 29 Dec 2018 17:15:52 +0000</pubDate><guid>https://kubernetes.ansilh.com/16-k8s_from_sratch/09-worker-plane/index.html</guid><description>Bootstrapping worker nodes In this lab , we will bootstrap three worker nodes. The following components will be installed on each node.
Docker Kubelet Kube-proxy Calico Network Plugin CoreDNS Docker Instructions are here
Once docker is installed , execute below steps to make docker ready for kubelet integration.
$ sudo vi /lib/systemd/system/docker.serviceDisable iptables, default bridge network and masquerading on docker
ExecStart=/usr/bin/dockerd -H fd:// --bridge=none --iptables=false --ip-masq=falseCleanup all docker specific networking from worker nodes</description></item></channel></rss>